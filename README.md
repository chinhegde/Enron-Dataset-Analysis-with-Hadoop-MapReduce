# Enron-Dataset-Analysis-with-Hadoop-MapReduce
Frequent Itemsets are found for 3 classifications of data - Line as unit, Sentence as unit & Paragraph as a unit.

Steps:
1. 1. Get all email data from the given directories into 1 file -> emails.txt
2. Create datasets for mappers & reduces for each part of the exercise:
  a. Line as unit
  b. Sentence as unit
  c. Paragraph as unit
3. Execute MapReduce for all text files
4. Execute FPG using MAHOUT for all text files
5. Compare the outputs.

Hadoop MapReduce has been executed through Google Colaboratory.
Colab allows us to write and execute Python code through the browser.
* We can use upto 88.6 GB
* We can access our Google Drive (which acts as our local memory)


